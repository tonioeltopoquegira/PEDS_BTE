What I would like you to do is the following:
1. Try to exploit the fact that model is batch-independent (computes kappa for 1 pores in basically the same time as for 200 pores). 
It would be stupid not exploiting this for optimization purposes. Try to run N independent optimizations at the same time, i.e. one step computes 200 gradients and performs 200 independent updates.
Currently I am using a stupid strategy in which I exploit this "perturbing" our current solution. Ignore that part and change it.

2. Try optimization under uncertainty! predict(model, pores) outputs in the case of the ensemble a tuple (kappa_mean, kappa_var). We would like to use this fact
and optimize not only for the value that is closest to kappa_target but also for the one with least variance (least model parameters uncertainty). 
I would try it just adding the variance to the cost function loss = (kappa_target - kappa_mean)**2 + kappa_var but feel free to try something different!

3. Currently we are optimizing in a "continuous relaxation" meaning that 25 parameters are binarized to 0-1 only after the optimization. Try to check the paper I sent you
and apply those constraints. You can also create a penalty for getting outside of 0-1 that still gives us some room to explore the space.



