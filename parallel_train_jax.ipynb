{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"  \n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import pmap\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "from modules.params_utils import save_params\n",
    "from modules.training_utils import data_loader, print_generated, update_and_check_grads, clip_gradients, plot_learning_curves, choose_schedule\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax import device_put\n",
    "import numpy as np\n",
    "\n",
    "from models.peds import PEDS\n",
    "from modules.params_utils import initialize_or_restore_params\n",
    "\n",
    "from modules.params_utils import initialize_or_restore_params\n",
    "from modules.training import train_model\n",
    "\n",
    "from models.mlp import mlp\n",
    "from models.cnn import cnn\n",
    "from solvers.low_fidelity_solvers.lowfidsolver_class import lowfid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Define Parallelization\n",
    "n_devices = len(jax.devices())\n",
    "\n",
    "print(n_devices)\n",
    "\n",
    "mesh = Mesh(devices=np.array(jax.devices()), axis_names=('devices',))\n",
    "data_sharding = NamedSharding(mesh, PartitionSpec('devices',))\n",
    "\n",
    "# Ingest data <- Here we will do active learning\n",
    "full_data = jnp.load(\"data/highfidelity/high_fidelity_10012_20steps.npz\", allow_pickle=True)\n",
    "\n",
    "pores = jnp.asarray(full_data['pores'], dtype=jnp.float32)\n",
    "kappas = jnp.asarray(full_data['kappas'], dtype=jnp.float32)\n",
    "base_conductivities = jnp.asarray(full_data['conductivity'], dtype=jnp.float32)\n",
    "\n",
    "# Create dataset\n",
    "dataset_train = [pores[:8000], base_conductivities[:8000], kappas[:8000]]\n",
    "dataset_valid = [pores[8000:], base_conductivities[8000:], kappas[8000:]]\n",
    "\n",
    "# Shard the dataset\n",
    "def shard_dataset(dataset, n_device, sharding):\n",
    "    # Extract components of the dataset\n",
    "    pores, conductivities, kappas = dataset\n",
    "    \n",
    "    # Determine shard sizes\n",
    "    shard_size = pores.shape[0] // n_devices\n",
    "    \n",
    "    # Ensure each shard is of equal size\n",
    "    assert pores.shape[0] % n_devices == 0, \"Dataset size must be divisible by the number of devices\"\n",
    "    \n",
    "    # Reshape data to distribute across devices\n",
    "    pores_sharded = pores.reshape(n_devices, shard_size, *pores.shape[1:])\n",
    "    conductivities_sharded = conductivities.reshape(n_devices, shard_size, *conductivities.shape[1:])\n",
    "    kappas_sharded = kappas.reshape(n_devices, shard_size, *kappas.shape[1:])\n",
    "\n",
    "    # Apply NamedSharding to the reshaped data\n",
    "    pores_sharded = device_put(pores_sharded, sharding)\n",
    "    conductivities_sharded = device_put(conductivities_sharded, sharding)\n",
    "    kappas_sharded = device_put(kappas_sharded, sharding)\n",
    "\n",
    "    return (pores_sharded, conductivities_sharded, kappas_sharded)\n",
    "\n",
    "\n",
    "# Shard training and validation datasets\n",
    "dataset_train = shard_dataset(dataset_train, n_devices, data_sharding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(*arrays, batch_size):\n",
    "    \n",
    "    # Ensure all arrays have the same number of samples\n",
    "    n_samples = arrays[1].shape[1]\n",
    "    for array in arrays:\n",
    "        assert array.shape[1] == n_samples, \"All input arrays must have the same first dimension or second in the case of parallelized.\"\n",
    "    \n",
    "    indices = jnp.arange(n_samples)  # Use jnp.arange for JAX arrays\n",
    "    \n",
    "    # Split into batches and yield\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        yield tuple(array[:, batch_indices] for array in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Initializing new parameters.\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "key = nnx.Rngs(42)\n",
    "generator = mlp(input_size= 25, hidden_sizes=[32, 64, 128], step_size=5, rngs=key)\n",
    "#generator = cnn(rngs=key)\n",
    "\n",
    "# Params initializing or restoring\n",
    "generator, checkpointer, ckpt_dir = initialize_or_restore_params(generator, model_name='peds_PI')\n",
    "\n",
    "# Low Fidelity Solver\n",
    "lowfidsolver = lowfid(solver='gauss', iterations=1000)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "schedule = \"constant\"\n",
    "learn_rate_min = 5e-5\n",
    "learn_rate_max = 5e-5\n",
    "batch_size = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(generator, lowfidsolver, pores, conductivities):\n",
    "\n",
    "    conductivity_res = nnx.jit(generator)(pores)\n",
    "        \n",
    "    new_conductivity = conductivity_res+conductivities \n",
    "\n",
    "    new_conductivity = jnp.maximum(new_conductivity, 1e-5) # here we \n",
    "    \n",
    "    kappa = lowfidsolver(new_conductivity) \n",
    "    \n",
    "    return kappa, conductivity_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.tree_util import tree_map\n",
    "\n",
    "def simplify_grad_structure(grads):\n",
    "    def simplify_state(var):\n",
    "        var = jnp.squeeze(var[:1], 0)  \n",
    "        return var\n",
    "    # Apply the simplification to the entire gradient tree\n",
    "    return tree_map(simplify_state, grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = choose_schedule(schedule, learn_rate_min, learn_rate_max, epochs)\n",
    "optimizer = nnx.Optimizer(generator, optax.adam(lr_schedule))\n",
    "\n",
    "def train_step(pores, conductivities, kappas, batch_n, epoch): # sharded pores and kappas\n",
    "    \n",
    "    def loss_fn(generator):\n",
    "        \n",
    "        kappa_pred, conductivity_res = predict(generator, lowfidsolver, pores, conductivities)\n",
    "        residuals = (kappa_pred - kappas)\n",
    "\n",
    "        return jnp.sum(residuals**2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(generator)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "@partial(\n",
    "pmap,\n",
    "axis_name='devices',\n",
    "static_broadcasted_argnums=(3, 4)  # Indices of `batch_n` and `epoch`\n",
    ")\n",
    "def parallel_train_step(pores, conductivities, kappas, batch_n, epoch):\n",
    "    # `train_step` must return loss and gradients\n",
    "    loss, grads = train_step(pores, conductivities, kappas, batch_n, epoch)\n",
    "\n",
    "    #return loss, grads\n",
    "    grads_tot =  jax.lax.psum(grads, axis_name='devices')\n",
    "    loss_tot =  jax.lax.psum(loss, axis_name='devices')\n",
    "\n",
    "    return loss_tot, grads_tot\n",
    "\n",
    "# Function to accumulate gradients\n",
    "def accumulate_gradients(total_grads, new_grads):\n",
    "    if total_grads is None:\n",
    "        return new_grads\n",
    "    return jax.tree_util.tree_map(lambda x, y: x + y, total_grads, new_grads)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "epoch_losses = np.zeros(epochs) # \n",
    "valid_losses = np.zeros(epochs)\n",
    "valid_perc_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    grads = None\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "    \n",
    "\n",
    "    #batch_size = batch_size // n_devices\n",
    "\n",
    "    for en, batch in enumerate(data_loader(*dataset_train, batch_size=batch_size)):\n",
    "        \n",
    "        pores_sharded, conductivities_sharded, kappas_sharded = batch\n",
    "        \n",
    "        # Perform parallel computation of loss and gradients\n",
    "        losses, new_grads = parallel_train_step(pores_sharded, conductivities_sharded, kappas_sharded, en, epoch)\n",
    "\n",
    "        # Print the first element of the first bias gradient (before reduction)\n",
    "\n",
    "        # Accumulate gradients across batches\n",
    "        grads = accumulate_gradients(grads, new_grads)\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += jnp.sum(losses)  # Sum losses across devices\n",
    "    \n",
    "    avg_loss = total_loss / dataset_train[0].shape[0]\n",
    "\n",
    "    #avg_val_loss, total_loss_perc = valid(dataset_valid, batch_size, generator, lowfidsolver)\n",
    "    avg_val_loss, total_loss_perc = 0.0, 0.0\n",
    "\n",
    "    # Print the average loss at the end of each epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.2f}, Validation Losses: [{avg_val_loss:.2f}, {total_loss_perc:.2f}%], Epoch time: {time.time() - epoch_time:.2f}s\")\n",
    "\n",
    "    # per ognuna delle variabili (in ['layers'] --> ['kernel'] e ['bias'], prendi solamente la prima e scarta le altre 8)\n",
    "    #grads = extract_first_elements(grads)\n",
    "    \n",
    "    grads = simplify_grad_structure(grads)\n",
    "\n",
    "    optimizer.update(grads)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
