{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"  \n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import pmap\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax import nnx\n",
    "\n",
    "from modules.params_utils import save_params\n",
    "from modules.training_utils import data_loader, print_generated, update_and_check_grads, clip_gradients, plot_learning_curves, choose_schedule\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import nnx\n",
    "from jax.sharding import Mesh, PartitionSpec, NamedSharding\n",
    "from jax import device_put\n",
    "import numpy as np\n",
    "\n",
    "from models.peds import PEDS\n",
    "from modules.params_utils import initialize_or_restore_params\n",
    "\n",
    "from modules.params_utils import initialize_or_restore_params\n",
    "from modules.training import train_model\n",
    "\n",
    "from models.mlp import mlp\n",
    "from models.cnn import cnn\n",
    "from solvers.low_fidelity_solvers.lowfidsolver_class import lowfid\n",
    "\n",
    "from mpi4py import MPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process 0 of 1 initialized\n",
      "Jax devices 8\n",
      "(8000, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "# Define Parallelization\n",
    "n_devices = len(jax.devices())\n",
    "\n",
    "# Initialize MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()  # Current process ID\n",
    "size = comm.Get_size()  # Total number of processes\n",
    "\n",
    "print(f\"Process {rank} of {size} initialized\")\n",
    "\n",
    "print(\"Jax devices\", n_devices)\n",
    "\n",
    "mesh = Mesh(devices=np.array(jax.devices()), axis_names=('devices',))\n",
    "data_sharding = NamedSharding(mesh, PartitionSpec('devices',))\n",
    "\n",
    "# Ingest data <- Here we will do active learning\n",
    "full_data = jnp.load(\"data/highfidelity/high_fidelity_10012_20steps.npz\", allow_pickle=True)\n",
    "\n",
    "pores = jnp.asarray(full_data['pores'], dtype=jnp.float32)\n",
    "kappas = jnp.asarray(full_data['kappas'], dtype=jnp.float32)\n",
    "base_conductivities = jnp.asarray(full_data['conductivity'], dtype=jnp.float32)\n",
    "\n",
    "# Create dataset\n",
    "dataset_train = [pores[:8000], base_conductivities[:8000], kappas[:8000]]\n",
    "dataset_valid = [pores[8000:], base_conductivities[8000:], kappas[8000:]]\n",
    "\n",
    "# Data distribution among MPI ranks\n",
    "def distribute_dataset(dataset, rank, size):\n",
    "    \"\"\"\n",
    "    Distribute dataset among MPI ranks.\n",
    "\n",
    "    Args:\n",
    "    - dataset: List of arrays [pores, conductivities, kappas].\n",
    "    - rank: Current MPI rank.\n",
    "    - size: Total number of MPI processes.\n",
    "\n",
    "    Returns:\n",
    "    - Local dataset for this rank.\n",
    "    \"\"\"\n",
    "    pores, conductivities, kappas = dataset\n",
    "\n",
    "    # Determine chunk size per rank\n",
    "    n_samples = pores.shape[0]\n",
    "    chunk_size = n_samples // size\n",
    "\n",
    "    assert n_samples % size == 0, \"Dataset size must be divisible by the number of MPI processes\"\n",
    "\n",
    "    # Compute local data slice\n",
    "    start_idx = rank * chunk_size\n",
    "    end_idx = start_idx + chunk_size\n",
    "\n",
    "    # Slice the dataset for this rank\n",
    "    local_pores = pores[start_idx:end_idx]\n",
    "    local_conductivities = conductivities[start_idx:end_idx]\n",
    "    local_kappas = kappas[start_idx:end_idx]\n",
    "\n",
    "    return [local_pores, local_conductivities, local_kappas]\n",
    "\n",
    "# Distribute the training and validation datasets\n",
    "dataset_train_local = distribute_dataset(dataset_train, rank, size)\n",
    "dataset_valid_local = distribute_dataset(dataset_valid, rank, size)\n",
    "\n",
    "pores, cond, kappas = dataset_train_local\n",
    "\n",
    "print(pores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader_local(*arrays, batch_size):\n",
    "    \"\"\"\n",
    "    Local data loader for distributed training.\n",
    "\n",
    "    Args:\n",
    "    - arrays: Local dataset arrays (e.g., pores, conductivities, kappas).\n",
    "    - batch_size: Size of each batch.\n",
    "\n",
    "    Yields:\n",
    "    - Batches of data for the local rank.\n",
    "    \"\"\"\n",
    "    n_samples = arrays[0].shape[0]\n",
    "    indices = np.arange(n_samples)  # Use NumPy for indices to avoid JAX compatibility issues\n",
    "\n",
    "    # Yield batches\n",
    "    for start_idx in range(0, n_samples, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        yield tuple(array[batch_indices] for array in arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. Initializing new parameters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create model\n",
    "key = nnx.Rngs(42)\n",
    "generator = mlp(input_size= 25, hidden_sizes=[32, 64, 128], step_size=5, rngs=key)\n",
    "#generator = cnn(rngs=key)\n",
    "\n",
    "# Params initializing or restoring\n",
    "generator, checkpointer, ckpt_dir = initialize_or_restore_params(generator, model_name='peds_PI')\n",
    "\n",
    "# Low Fidelity Solver\n",
    "lowfidsolver = lowfid(solver='direct', iterations=1000)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "schedule = \"constant\"\n",
    "learn_rate_min = 5e-5\n",
    "learn_rate_max = 5e-5\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(generator, lowfidsolver, pores, conductivities):\n",
    "\n",
    "    conductivity_res = nnx.jit(generator)(pores)\n",
    "        \n",
    "    new_conductivity = conductivity_res+conductivities \n",
    "\n",
    "    new_conductivity = jnp.maximum(new_conductivity, 1e-5) # here we \n",
    "    \n",
    "    kappa = lowfidsolver(new_conductivity) \n",
    "    \n",
    "    return kappa, conductivity_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mpi_allreduce_gradients(local_grads):\n",
    "    # Perform MPI Allreduce to accumulate gradients across all ranks\n",
    "    return jax.tree_util.tree_map(\n",
    "        lambda x: comm.allreduce(x, op=MPI.SUM), local_grads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    }
   ],
   "source": [
    "lr_schedule = choose_schedule(schedule, learn_rate_min, learn_rate_max, epochs)\n",
    "optimizer = nnx.Optimizer(generator, optax.adam(lr_schedule))\n",
    "\n",
    "def train_step(pores, conductivities, kappas, batch_n, epoch): # sharded pores and kappas\n",
    "    \n",
    "    def loss_fn(generator):\n",
    "        \n",
    "        kappa_pred, conductivity_res = predict(generator, lowfidsolver, pores, conductivities)\n",
    "        residuals = (kappa_pred - kappas)\n",
    "\n",
    "        return jnp.sum(residuals**2)\n",
    "\n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(generator)\n",
    "    \n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "@partial(\n",
    "pmap,\n",
    "axis_name='devices',\n",
    "static_broadcasted_argnums=(3, 4)  # Indices of `batch_n` and `epoch`\n",
    ")\n",
    "def parallel_train_step(pores, conductivities, kappas, batch_n, epoch):\n",
    "    # `train_step` must return loss and gradients\n",
    "    loss, grads = train_step(pores, conductivities, kappas, batch_n, epoch)\n",
    "\n",
    "    #return loss, grads\n",
    "    grads_tot =  jax.lax.psum(grads, axis_name='devices')\n",
    "    loss_tot =  jax.lax.psum(loss, axis_name='devices')\n",
    "\n",
    "    return loss_tot, grads_tot\n",
    "\n",
    "# Function to accumulate gradients\n",
    "def accumulate_gradients(total_grads, new_grads):\n",
    "    if total_grads is None:\n",
    "        return new_grads\n",
    "    return jax.tree_util.tree_map(lambda x, y: x + y, total_grads, new_grads)\n",
    "\n",
    "print(\"Training...\")\n",
    "\n",
    "epoch_losses = np.zeros(epochs) # \n",
    "valid_losses = np.zeros(epochs)\n",
    "valid_perc_losses = np.zeros(epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    grads = None\n",
    "    total_loss = 0.0  # Initialize total loss for the epoch\n",
    "    \n",
    "\n",
    "    #batch_size = batch_size // n_devices\n",
    "\n",
    "    for en, batch in enumerate(data_loader(*dataset_train, batch_size=batch_size)):\n",
    "        \n",
    "        pores_sharded, conductivities_sharded, kappas_sharded = batch\n",
    "        \n",
    "        # Perform parallel computation of loss and gradients\n",
    "        losses, new_grads = parallel_train_step(pores_sharded, conductivities_sharded, kappas_sharded, en, epoch)\n",
    "\n",
    "        # Print the first element of the first bias gradient (before reduction)\n",
    "\n",
    "        # Accumulate gradients across batches\n",
    "        grads = accumulate_gradients(grads, new_grads)\n",
    "\n",
    "        # Accumulate loss\n",
    "        total_loss += jnp.sum(losses)  # Sum losses across devices\n",
    "    \n",
    "    avg_loss = total_loss / dataset_train[0].shape[0]\n",
    "\n",
    "    #avg_val_loss, total_loss_perc = valid(dataset_valid, batch_size, generator, lowfidsolver)\n",
    "    avg_val_loss, total_loss_perc = 0.0, 0.0\n",
    "\n",
    "    # Print the average loss at the end of each epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.2f}, Validation Losses: [{avg_val_loss:.2f}, {total_loss_perc:.2f}%], Epoch time: {time.time() - epoch_time:.2f}s\")\n",
    "\n",
    "    # per ognuna delle variabili (in ['layers'] --> ['kernel'] e ['bias'], prendi solamente la prima e scarta le altre 8)\n",
    "    #grads = extract_first_elements(grads)\n",
    "    \n",
    "    grads = simplify_grad_structure(grads)\n",
    "\n",
    "    optimizer.update(grads)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
